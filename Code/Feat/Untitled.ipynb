{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Generate distance features...\n",
      "generate unigram\n",
      "generate bigram\n",
      "generate trigram\n",
      "generate jaccard coef and dice dist for n-gram\n",
      "For cross-validation...\n",
      "Run: 1, Fold: 1\n",
      "Run: 1, Fold: 2\n",
      "Run: 1, Fold: 3\n",
      "Run: 2, Fold: 1\n",
      "Run: 2, Fold: 2\n",
      "Run: 2, Fold: 3\n",
      "Run: 3, Fold: 1\n",
      "Run: 3, Fold: 2\n",
      "Run: 3, Fold: 3\n",
      "Done.\n",
      "For training and testing...\n",
      "generate unigram\n",
      "generate bigram\n",
      "generate trigram\n",
      "generate jaccard coef and dice dist for n-gram\n",
      "Feature names are stored in ../../Feat/solution/distance.feat_name\n",
      "All Done.\n"
     ]
    }
   ],
   "source": [
    "# %load genFeat_distance_feat.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    genFeat_distance_feat.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file generates the following features for each run and fold, and for the entire training and testing set.\n",
    "\n",
    "        1. jaccard coefficient/dice distance between query & title, query & description, title & description pairs\n",
    "            - just plain jaccard coefficient/dice distance\n",
    "            - compute for unigram/bigram/trigram\n",
    "\n",
    "        2. jaccard coefficient/dice distance stats features for title/description\n",
    "            - computation is carried out with regard to a pool of samples grouped by:\n",
    "                - median_relevance (#4)\n",
    "                - query (qid) & median_relevance (#4)\n",
    "            - jaccard coefficient/dice distance for the following pairs are computed for each sample\n",
    "                - sample title        vs.  pooled sample titles\n",
    "                - sample description  vs.  pooled sample descriptions\n",
    "                Note that in the pool samples, we exclude the current sample being considered.\n",
    "            - stats features include quantiles of cosine similarity and others defined in the variable \"stats_func\", e.g.,\n",
    "                - mean value\n",
    "                - standard deviation (std)\n",
    "                - more can be added, e.g., moment features etc\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import ngram\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from nlp_utils import stopwords, english_stemmer, stem_tokens\n",
    "from feat_utils import try_divide, get_sample_indices_by_relevance, dump_feat_name\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "## stats feat is quite time-consuming to generate\n",
    "## (about 2 days to generate on my computer)\n",
    "stats_feat_flag = False\n",
    "\n",
    "\n",
    "#####################\n",
    "## Distance metric ##\n",
    "#####################\n",
    "def JaccardCoef(A, B):\n",
    "    A, B = set(A), set(B)\n",
    "    intersect = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    coef = try_divide(intersect, union)\n",
    "    return coef\n",
    "\n",
    "def DiceDist(A, B):\n",
    "    A, B = set(A), set(B)\n",
    "    intersect = len(A.intersection(B))\n",
    "    union = len(A) + len(B)\n",
    "    d = try_divide(2*intersect, union)\n",
    "    return d\n",
    "\n",
    "def compute_dist(A, B, dist=\"jaccard_coef\"):\n",
    "    if dist == \"jaccard_coef\":\n",
    "        d = JaccardCoef(A, B)\n",
    "    elif dist == \"dice_dist\":\n",
    "        d = DiceDist(A, B)\n",
    "    return d\n",
    "\n",
    "#### pairwise distance\n",
    "def pairwise_jaccard_coef(A, B):\n",
    "    coef = np.zeros((A.shape[0], B.shape[0]), dtype=float)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[0]):\n",
    "            coef[i,j] = JaccardCoef(A[i], B[j])\n",
    "    return coef\n",
    "    \n",
    "def pairwise_dice_dist(A, B):\n",
    "    d = np.zeros((A.shape[0], B.shape[0]), dtype=float)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[0]):\n",
    "            d[i,j] = DiceDist(A[i], B[j])\n",
    "    return d\n",
    "\n",
    "def pairwise_dist(A, B, dist=\"jaccard_coef\"):\n",
    "    if dist == \"jaccard_coef\":\n",
    "        d = pairwise_jaccard_coef(A, B)\n",
    "    elif dist == \"dice_dist\":\n",
    "        d = pairwise_dice_dist(A, B)\n",
    "    return d\n",
    "\n",
    "\n",
    "######################\n",
    "## Pre-process data ##\n",
    "######################\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "#token_pattern = r'\\w{1,}'\n",
    "#token_pattern = r\"\\w+\"\n",
    "#token_pattern = r\"[\\w']+\"\n",
    "transform = config.count_feat_transform\n",
    "def preprocess_data(line, token_pattern=token_pattern,\n",
    "                     exclude_stopword=config.cooccurrence_word_exclude_stopword,\n",
    "                     encode_digit=False):\n",
    "    token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)\n",
    "    ## tokenize\n",
    "    tokens = [x.lower() for x in token_pattern.findall(line)]\n",
    "    ## stem\n",
    "    tokens_stemmed = stem_tokens(tokens, english_stemmer)\n",
    "    if exclude_stopword:\n",
    "        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "#####################################\n",
    "## Extract basic distance features ##\n",
    "#####################################\n",
    "def extract_basic_distance_feat(df):\n",
    "    ## unigram\n",
    "    print \"generate unigram\"\n",
    "    df[\"query_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"query\"]), axis=1))\n",
    "    df[\"title_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_title\"]), axis=1))\n",
    "    df[\"description_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_description\"]), axis=1))\n",
    "    ## bigram\n",
    "    print \"generate bigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    ## trigram\n",
    "    print \"generate trigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "\n",
    "    ## jaccard coef/dice dist of n-gram\n",
    "    print \"generate jaccard coef and dice dist for n-gram\"\n",
    "    dists = [\"jaccard_coef\", \"dice_dist\"]\n",
    "    grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    feat_names = [\"query\", \"title\", \"description\"]\n",
    "    for dist in dists:\n",
    "        for gram in grams:\n",
    "            for i in range(len(feat_names)-1):\n",
    "                for j in range(i+1,len(feat_names)):\n",
    "                    target_name = feat_names[i]\n",
    "                    obs_name = feat_names[j]\n",
    "                    df[\"%s_of_%s_between_%s_%s\"%(dist,gram,target_name,obs_name)] = \\\n",
    "                            list(df.apply(lambda x: compute_dist(x[target_name+\"_\"+gram], x[obs_name+\"_\"+gram], dist), axis=1))\n",
    "\n",
    "\n",
    "###########################################\n",
    "## Extract statistical distance features ##\n",
    "###########################################\n",
    "## generate dist stats feat\n",
    "def generate_dist_stats_feat(dist, X_train, ids_train, X_test, ids_test, indices_dict, qids_test=None):\n",
    "\n",
    "    stats_feat = 0 * np.ones((len(ids_test), stats_feat_num*config.n_classes), dtype=float)\n",
    "    ## pairwise dist\n",
    "    distance = pairwise_dist(X_test, X_train, dist)\n",
    "    for i in range(len(ids_test)):\n",
    "        id = ids_test[i]\n",
    "        if qids_test is not None:\n",
    "            qid = qids_test[i]\n",
    "        for j in range(config.n_classes):\n",
    "            key = (qid, j+1) if qids_test is not None else j+1\n",
    "            if indices_dict.has_key(key):\n",
    "                inds = indices_dict[key]\n",
    "                # exclude this sample itself from the list of indices\n",
    "                inds = [ ind for ind in inds if id != ids_train[ind] ]\n",
    "                distance_tmp = distance[i][inds]\n",
    "                if len(distance_tmp) != 0:\n",
    "                    feat = [ func(distance_tmp) for func in stats_func ]\n",
    "                    ## quantile\n",
    "                    distance_tmp = pd.Series(distance_tmp)\n",
    "                    quantiles = distance_tmp.quantile(quantiles_range)\n",
    "                    feat = np.hstack((feat, quantiles))\n",
    "                    stats_feat[i,j*stats_feat_num:(j+1)*stats_feat_num] = feat\n",
    "    return stats_feat\n",
    "\n",
    "\n",
    "def extract_statistical_distance_feat(path, dfTrain, dfTest, mode, feat_names):\n",
    "\n",
    "    new_feat_names = copy(feat_names)\n",
    "    ## get the indices of pooled samples\n",
    "    relevance_indices_dict = get_sample_indices_by_relevance(dfTrain)\n",
    "    query_relevance_indices_dict = get_sample_indices_by_relevance(dfTrain, \"qid\")\n",
    "    ## very time consuming\n",
    "    for dist in [\"jaccard_coef\", \"dice_dist\"]:\n",
    "        for name in [\"title\", \"description\"]:\n",
    "            for gram in [\"unigram\", \"bigram\", \"trigram\"]:\n",
    "                ## train\n",
    "                dist_stats_feat_by_relevance_train = generate_dist_stats_feat(dist, dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                            dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                            relevance_indices_dict)\n",
    "                dist_stats_feat_by_query_relevance_train = generate_dist_stats_feat(dist, dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                                dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                                query_relevance_indices_dict, dfTrain[\"qid\"].values)\n",
    "                with open(\"%s/train.%s_%s_%s_stats_feat_by_relevance.feat.pkl\" % (path, name, gram, dist), \"wb\") as f:\n",
    "                    cPickle.dump(dist_stats_feat_by_relevance_train, f, -1)\n",
    "                with open(\"%s/train.%s_%s_%s_stats_feat_by_query_relevance.feat.pkl\" % (path, name, gram, dist), \"wb\") as f:\n",
    "                    cPickle.dump(dist_stats_feat_by_query_relevance_train, f, -1)\n",
    "                ## test\n",
    "                dist_stats_feat_by_relevance_test = generate_dist_stats_feat(dist, dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                            dfTest[name+\"_\"+gram].values, dfTest[\"id\"].values,\n",
    "                                                            relevance_indices_dict)\n",
    "                dist_stats_feat_by_query_relevance_test = generate_dist_stats_feat(dist, dfTrain[name+\"_\"+gram].values, dfTrain[\"id\"].values,\n",
    "                                                                dfTest[name+\"_\"+gram].values, dfTest[\"id\"].values,\n",
    "                                                                query_relevance_indices_dict, dfTest[\"qid\"].values)\n",
    "                with open(\"%s/%s.%s_%s_%s_stats_feat_by_relevance.feat.pkl\" % (path, mode, name, gram, dist), \"wb\") as f:\n",
    "                    cPickle.dump(dist_stats_feat_by_relevance_test, f, -1)\n",
    "                with open(\"%s/%s.%s_%s_%s_stats_feat_by_query_relevance.feat.pkl\" % (path, mode, name, gram, dist), \"wb\") as f:\n",
    "                    cPickle.dump(dist_stats_feat_by_query_relevance_test, f, -1)\n",
    "\n",
    "                ## update feat names\n",
    "                new_feat_names.append( \"%s_%s_%s_stats_feat_by_relevance\" % (name, gram, dist) )\n",
    "                new_feat_names.append( \"%s_%s_%s_stats_feat_by_query_relevance\" % (name, gram, dist) )\n",
    "\n",
    "    return new_feat_names\n",
    "\n",
    "\n",
    "##########\n",
    "## Main ##\n",
    "##########\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ###############\n",
    "    ## Load Data ##\n",
    "    ###############\n",
    "    ## load data\n",
    "    with open(config.processed_train_data_path, \"rb\") as f:\n",
    "        dfTrain = cPickle.load(f)\n",
    "    with open(config.processed_test_data_path, \"rb\") as f:\n",
    "        dfTest = cPickle.load(f)\n",
    "    ## load pre-defined stratified k-fold index\n",
    "    with open(\"%s/stratifiedKFold.%s.pkl\" % (config.data_folder, config.stratified_label), \"rb\") as f:\n",
    "            skf = cPickle.load(f)\n",
    "\n",
    "\n",
    "    ## file to save feat names\n",
    "    feat_name_file = \"%s/distance.feat_name\" % config.feat_folder\n",
    "\n",
    "\n",
    "    ## stats to extract\n",
    "    quantiles_range = np.arange(0, 1.5, 0.5)\n",
    "    stats_func = [ np.mean, np.std ]\n",
    "    stats_feat_num = len(quantiles_range) + len(stats_func)\n",
    "\n",
    "    #######################\n",
    "    ## Generate Features ##\n",
    "    #######################\n",
    "    print(\"==================================================\")\n",
    "    print(\"Generate distance features...\")\n",
    "\n",
    "    extract_basic_distance_feat(dfTrain)\n",
    "    feat_names = [name for name in dfTrain.columns if \"jaccard_coef\" in name or \"dice_dist\" in name]\n",
    "\n",
    "    print(\"For cross-validation...\")\n",
    "    for run in range(config.n_runs):\n",
    "        ## use 33% for training and 67 % for validation\n",
    "        ## so we switch trainInd and validInd\n",
    "        for fold, (validInd, trainInd) in enumerate(skf[run]):\n",
    "            print(\"Run: %d, Fold: %d\" % (run+1, fold+1))\n",
    "            path = \"%s/Run%d/Fold%d\" % (config.feat_folder, run+1, fold+1)\n",
    "              \n",
    "            for feat_name in feat_names:\n",
    "                X_train = dfTrain[feat_name].values[trainInd]\n",
    "                X_valid = dfTrain[feat_name].values[validInd]\n",
    "                with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_train, f, -1)\n",
    "                with open(\"%s/valid.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_valid, f, -1)\n",
    "            ## extract statistical distance features\n",
    "            if stats_feat_flag:\n",
    "                dfTrain2 = dfTrain.iloc[trainInd].copy()\n",
    "                dfValid = dfTrain.iloc[validInd].copy()\n",
    "                extract_statistical_distance_feat(path, dfTrain2, dfValid, \"valid\", feat_names)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "    print(\"For training and testing...\")\n",
    "    path = \"%s/All\" % config.feat_folder\n",
    "    ## use full version for X_train\n",
    "    extract_basic_distance_feat(dfTest)\n",
    "    for feat_name in feat_names:\n",
    "        X_train = dfTrain[feat_name].values\n",
    "        X_test = dfTest[feat_name].values\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_train, f, -1)\n",
    "        with open(\"%s/test.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_test, f, -1)\n",
    "    ## extract statistical distance features\n",
    "    if stats_feat_flag:\n",
    "        feat_names = extract_statistical_distance_feat(path, dfTrain, dfTest, \"test\", feat_names)\n",
    "\n",
    "    ## save feat names\n",
    "    print(\"Feature names are stored in %s\" % feat_name_file)\n",
    "    ## dump feat name\n",
    "    dump_feat_name(feat_names, feat_name_file)\n",
    "            \n",
    "    print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
