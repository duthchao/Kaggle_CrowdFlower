{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load run_all.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "\trun_all.py\n",
    "\n",
    "__description___\n",
    "\t\n",
    "\tThis file generates all the features in one shot.\n",
    "\n",
    "__author__\n",
    "\n",
    "\tChenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "#################\n",
    "## Preprocesss ##\n",
    "#################\n",
    "#### preprocess data\n",
    "cmd = \"python ./preprocess.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "# #### generate kfold\n",
    "# cmd = \"python ./gen_kfold.py\"\n",
    "# os.system(cmd)\n",
    "\n",
    "#######################\n",
    "## Generate features ##\n",
    "#######################\n",
    "#### query id\n",
    "cmd = \"python ./genFeat_id_feat.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### counting feat\n",
    "cmd = \"python ./genFeat_counting_feat.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### distance feat\n",
    "cmd = \"python ./genFeat_distance_feat.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### basic tfidf\n",
    "cmd = \"python ./genFeat_basic_tfidf_feat.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### cooccurrence tfidf\n",
    "cmd = \"python ./genFeat_cooccurrence_tfidf_feat.py\"\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "#####################\n",
    "## Combine Feature ##\n",
    "#####################\n",
    "#### combine feat\n",
    "cmd = \"python ./combine_feat_[LSA_and_stats_feat_Jun09]_[Low].py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### combine feat\n",
    "cmd = \"python ./combine_feat_[LSA_svd150_and_Jaccard_coef_Jun14]_[Low].py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### combine feat\n",
    "cmd = \"python ./combine_feat_[svd100_and_bow_Jun23]_[Low].py\"\n",
    "os.system(cmd)\n",
    "\n",
    "#### combine feat\n",
    "cmd = \"python ./combine_feat_[svd100_and_bow_Jun27]_[High].py\"\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Done."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/IPython/kernel/__main__.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65497012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65516012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/6552101\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65527\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januarya/14146012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-process data...\n",
      "Done.\n",
      "Save data...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## pos tag text\\ndfTrain = dfTrain.apply(pos_tag_text, axis=1)\\ndfTest = dfTest.apply(pos_tag_text, axis=1)\\nwith open(config.pos_tagged_train_data_path, \"wb\") as f:\\n    cPickle.dump(dfTrain, f, -1)\\nwith open(config.pos_tagged_test_data_path, \"wb\") as f:\\n    cPickle.dump(dfTest, f, -1)\\nprint(\"Done.\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load preprocess.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    preprocess.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file preprocesses data.\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nlp_utils import clean_text, pos_tag_text\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "###############\n",
    "## Load Data ##\n",
    "###############\n",
    "print(\"Load data...\")\n",
    "\n",
    "dfTrain = pd.read_csv(config.original_train_data_path).fillna(\"\")\n",
    "dfTest = pd.read_csv(config.original_test_data_path).fillna(\"\")\n",
    "# number of train/test samples\n",
    "num_train, num_test = dfTrain.shape[0], dfTest.shape[0]\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "######################\n",
    "## Pre-process Data ##\n",
    "######################\n",
    "print(\"Pre-process data...\")\n",
    "\n",
    "## insert fake label for test\n",
    "dfTest[\"median_relevance\"] = np.ones((num_test))\n",
    "dfTest[\"relevance_variance\"] = np.zeros((num_test))\n",
    "\n",
    "## insert sample index\n",
    "dfTrain[\"index\"] = np.arange(num_train)\n",
    "dfTest[\"index\"] = np.arange(num_test)\n",
    "\n",
    "## one-hot encode the median_relevance\n",
    "for i in range(config.n_classes):\n",
    "    dfTrain[\"median_relevance_%d\" % (i+1)] = 0\n",
    "    dfTrain[\"median_relevance_%d\" % (i+1)][dfTrain[\"median_relevance\"]==(i+1)] = 1\n",
    "    \n",
    "## query ids\n",
    "qid_dict = dict()\n",
    "for i,q in enumerate(np.unique(dfTrain[\"query\"]), start=1):\n",
    "    qid_dict[q] = i\n",
    "    \n",
    "## insert query id\n",
    "dfTrain[\"qid\"] = map(lambda q: qid_dict[q], dfTrain[\"query\"])\n",
    "dfTest[\"qid\"] = map(lambda q: qid_dict[q], dfTest[\"query\"])\n",
    "\n",
    "## clean text\n",
    "clean = lambda line: clean_text(line, drop_html_flag=config.drop_html_flag)\n",
    "dfTrain = dfTrain.apply(clean, axis=1)\n",
    "dfTest = dfTest.apply(clean, axis=1)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "###############\n",
    "## Save Data ##\n",
    "###############\n",
    "print(\"Save data...\")\n",
    "\n",
    "with open(config.processed_train_data_path, \"wb\") as f:\n",
    "    cPickle.dump(dfTrain, f, -1)\n",
    "with open(config.processed_test_data_path, \"wb\") as f:\n",
    "    cPickle.dump(dfTest, f, -1)\n",
    "    \n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## pos tag text\n",
    "dfTrain = dfTrain.apply(pos_tag_text, axis=1)\n",
    "dfTest = dfTest.apply(pos_tag_text, axis=1)\n",
    "with open(config.pos_tagged_train_data_path, \"wb\") as f:\n",
    "    cPickle.dump(dfTrain, f, -1)\n",
    "with open(config.pos_tagged_test_data_path, \"wb\") as f:\n",
    "    cPickle.dump(dfTest, f, -1)\n",
    "print(\"Done.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Generate counting features...\n",
      "generate unigram\n",
      "generate bigram\n",
      "generate trigram\n",
      "generate word counting features\n",
      "generate intersect word counting features\n",
      "generate intersect word position features\n",
      "For cross-validation...\n",
      "Run: 1, Fold: 1\n",
      "Run: 1, Fold: 2\n",
      "Run: 1, Fold: 3\n",
      "Run: 2, Fold: 1\n",
      "Run: 2, Fold: 2\n",
      "Run: 2, Fold: 3\n",
      "Run: 3, Fold: 1\n",
      "Run: 3, Fold: 2\n",
      "Run: 3, Fold: 3\n",
      "Done.\n",
      "For training and testing...\n",
      "generate unigram\n",
      "generate bigram\n",
      "generate trigram\n",
      "generate word counting features\n",
      "generate intersect word counting features\n",
      "generate intersect word position features"
     ]
    }
   ],
   "source": [
    "# %load genFeat_counting_feat.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    genFeat_counting_feat.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file generates the following features for each run and fold, and for the entire training and testing set.\n",
    "\n",
    "        1. Basic Counting Features\n",
    "            \n",
    "            1. Count of n-gram in query/title/description\n",
    "\n",
    "            2. Count & Ratio of Digit in query/title/description\n",
    "\n",
    "            3. Count & Ratio of Unique n-gram in query/title/description\n",
    "\n",
    "        2. Intersect Counting Features\n",
    "\n",
    "            1. Count & Ratio of a's n-gram in b's n-gram\n",
    "\n",
    "        3. Intersect Position Features\n",
    "\n",
    "            1. Statistics of Positions of a's n-gram in b's n-gram\n",
    "\n",
    "            2. Statistics of Normalized Positions of a's n-gram in b's n-gram\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import ngram\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from nlp_utils import stopwords, english_stemmer, stem_tokens\n",
    "from feat_utils import try_divide, dump_feat_name\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "\n",
    "\n",
    "def get_position_list(target, obs):\n",
    "    \"\"\"\n",
    "        Get the list of positions of obs in target\n",
    "    \"\"\"\n",
    "    pos_of_obs_in_target = [0]\n",
    "    if len(obs) != 0:\n",
    "        pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\n",
    "        if len(pos_of_obs_in_target) == 0:\n",
    "            pos_of_obs_in_target = [0]\n",
    "    return pos_of_obs_in_target\n",
    "\n",
    "\n",
    "######################\n",
    "## Pre-process data ##\n",
    "######################\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "#token_pattern = r'\\w{1,}'\n",
    "#token_pattern = r\"\\w+\"\n",
    "#token_pattern = r\"[\\w']+\"\n",
    "def preprocess_data(line,\n",
    "                    token_pattern=token_pattern,\n",
    "                    exclude_stopword=config.cooccurrence_word_exclude_stopword,\n",
    "                    encode_digit=False):\n",
    "    token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)\n",
    "    ## tokenize\n",
    "    tokens = [x.lower() for x in token_pattern.findall(line)]\n",
    "    ## stem\n",
    "    tokens_stemmed = stem_tokens(tokens, english_stemmer)\n",
    "    if exclude_stopword:\n",
    "        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "def extract_feat(df):\n",
    "    ## unigram\n",
    "    print \"generate unigram\"\n",
    "    df[\"query_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"query\"]), axis=1))\n",
    "    df[\"title_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_title\"]), axis=1))\n",
    "    df[\"description_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_description\"]), axis=1))\n",
    "    ## bigram\n",
    "    print \"generate bigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    ## trigram\n",
    "    print \"generate trigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "\n",
    "\n",
    "    ################################\n",
    "    ## word count and digit count ##\n",
    "    ################################\n",
    "    print \"generate word counting features\"\n",
    "    feat_names = [\"query\", \"title\", \"description\"]\n",
    "    grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    count_digit = lambda x: sum([1. for w in x if w.isdigit()])\n",
    "    for feat_name in feat_names:\n",
    "        for gram in grams:\n",
    "            ## word count\n",
    "            df[\"count_of_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(x[feat_name+\"_\"+gram]), axis=1))\n",
    "            df[\"count_of_unique_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(set(x[feat_name+\"_\"+gram])), axis=1))\n",
    "            df[\"ratio_of_unique_%s_%s\"%(feat_name,gram)] = map(try_divide, df[\"count_of_unique_%s_%s\"%(feat_name,gram)], df[\"count_of_%s_%s\"%(feat_name,gram)])\n",
    "\n",
    "        ## digit count\n",
    "        df[\"count_of_digit_in_%s\"%feat_name] = list(df.apply(lambda x: count_digit(x[feat_name+\"_unigram\"]), axis=1))\n",
    "        df[\"ratio_of_digit_in_%s\"%feat_name] = map(try_divide, df[\"count_of_digit_in_%s\"%feat_name], df[\"count_of_%s_unigram\"%(feat_name)])\n",
    "\n",
    "    ## description missing indicator\n",
    "    df[\"description_missing\"] = list(df.apply(lambda x: int(x[\"description_unigram\"] == \"\"), axis=1))\n",
    "\n",
    "\n",
    "    ##############################\n",
    "    ## intersect word count ##\n",
    "    ##############################\n",
    "    print \"generate intersect word counting features\"\n",
    "    #### unigram\n",
    "    for gram in grams:\n",
    "        for obs_name in feat_names:\n",
    "            for target_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    ## query\n",
    "                    df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = list(df.apply(lambda x: sum([1. for w in x[obs_name+\"_\"+gram] if w in set(x[target_name+\"_\"+gram])]), axis=1))\n",
    "                    df[\"ratio_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = map(try_divide, df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)], df[\"count_of_%s_%s\"%(obs_name,gram)])\n",
    "\n",
    "        ## some other feat\n",
    "        df[\"title_%s_in_query_div_query_%s\"%(gram,gram)] = map(try_divide, df[\"count_of_title_%s_in_query\"%gram], df[\"count_of_query_%s\"%gram])\n",
    "        df[\"title_%s_in_query_div_query_%s_in_title\"%(gram,gram)] = map(try_divide, df[\"count_of_title_%s_in_query\"%gram], df[\"count_of_query_%s_in_title\"%gram])\n",
    "        df[\"description_%s_in_query_div_query_%s\"%(gram,gram)] = map(try_divide, df[\"count_of_description_%s_in_query\"%gram], df[\"count_of_query_%s\"%gram])\n",
    "        df[\"description_%s_in_query_div_query_%s_in_description\"%(gram,gram)] = map(try_divide, df[\"count_of_description_%s_in_query\"%gram], df[\"count_of_query_%s_in_description\"%gram])\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ## intersect word position feat ##\n",
    "    ######################################\n",
    "    print \"generate intersect word position features\"\n",
    "    for gram in grams:\n",
    "        for target_name in feat_names:\n",
    "            for obs_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    pos = list(df.apply(lambda x: get_position_list(x[target_name+\"_\"+gram], obs=x[obs_name+\"_\"+gram]), axis=1))\n",
    "                    ## stats feat on pos\n",
    "                    df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(np.min, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(np.mean, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(np.median, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(np.max, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(np.std, pos)\n",
    "                    ## stats feat on normalized_pos\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] , df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ###############\n",
    "    ## Load Data ##\n",
    "    ###############\n",
    "    ## load data\n",
    "    with open(config.processed_train_data_path, \"rb\") as f:\n",
    "        dfTrain = cPickle.load(f)\n",
    "    with open(config.processed_test_data_path, \"rb\") as f:\n",
    "        dfTest = cPickle.load(f)\n",
    "    ## load pre-defined stratified k-fold index\n",
    "    with open(\"%s/stratifiedKFold.%s.pkl\" % (config.data_folder, config.stratified_label), \"rb\") as f:\n",
    "            skf = cPickle.load(f)\n",
    "\n",
    "    ## file to save feat names\n",
    "    feat_name_file = \"%s/counting.feat_name\" % config.feat_folder\n",
    "\n",
    "\n",
    "    #######################\n",
    "    ## Generate Features ##\n",
    "    #######################\n",
    "    print(\"==================================================\")\n",
    "    print(\"Generate counting features...\")\n",
    "\n",
    "\n",
    "    extract_feat(dfTrain)\n",
    "    feat_names = [\n",
    "        name for name in dfTrain.columns \\\n",
    "            if \"count\" in name \\\n",
    "            or \"ratio\" in name \\\n",
    "            or \"div\" in name \\\n",
    "            or \"pos_of\" in name\n",
    "    ]\n",
    "    feat_names.append(\"description_missing\")\n",
    "\n",
    "\n",
    "    print(\"For cross-validation...\")\n",
    "    for run in range(config.n_runs):\n",
    "        ## use 33% for training and 67 % for validation\n",
    "        ## so we switch trainInd and validInd\n",
    "        for fold, (validInd, trainInd) in enumerate(skf[run]):\n",
    "            print(\"Run: %d, Fold: %d\" % (run+1, fold+1))\n",
    "            path = \"%s/Run%d/Fold%d\" % (config.feat_folder, run+1, fold+1)\n",
    "              \n",
    "            #########################\n",
    "            ## get word count feat ##\n",
    "            #########################\n",
    "            for feat_name in feat_names:\n",
    "                X_train = dfTrain[feat_name].values[trainInd]\n",
    "                X_valid = dfTrain[feat_name].values[validInd]\n",
    "                with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_train, f, -1)\n",
    "                with open(\"%s/valid.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_valid, f, -1)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "    print(\"For training and testing...\")\n",
    "    path = \"%s/All\" % config.feat_folder\n",
    "    ## use full version for X_train\n",
    "    extract_feat(dfTest)\n",
    "    for feat_name in feat_names:\n",
    "        X_train = dfTrain[feat_name].values\n",
    "        X_test = dfTest[feat_name].values\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_train, f, -1)\n",
    "        with open(\"%s/test.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_test, f, -1)\n",
    "            \n",
    "    ## save feat names\n",
    "    print(\"Feature names are stored in %s\" % feat_name_file)\n",
    "    ## dump feat name\n",
    "    dump_feat_name(feat_names, feat_name_file)\n",
    "\n",
    "    print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load genFeat_basic_tfidf_feat.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    genFeat_basic_tfidf_feat.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file generates the following features for each run and fold, and for the entire training and testing set.\n",
    "\n",
    "        1. basic tfidf features for query/title/description\n",
    "            - use common vocabulary among query/title/description for further computation of cosine similarity\n",
    "\n",
    "        2. cosine similarity between query & title, query & description, title & description pairs\n",
    "            - just plain cosine similarity\n",
    "\n",
    "        3. cosine similarity stats features for title/description\n",
    "            - computation is carried out with regard to a pool of samples grouped by:\n",
    "                - median_relevance (#4)\n",
    "                - query (qid) & median_relevance (#4)\n",
    "            - cosine similarity for the following pairs are computed for each sample\n",
    "                - sample title        vs.  pooled sample titles\n",
    "                - sample description  vs.  pooled sample descriptions\n",
    "                Note that in the pool samples, we exclude the current sample being considered.\n",
    "            - stats features include quantiles of cosine similarity and others defined in the variable \"stats_func\", e.g.,\n",
    "                - mean value\n",
    "                - standard deviation (std)\n",
    "                - more can be added, e.g., moment features etc\n",
    "\n",
    "        4. SVD version of the above features\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from scipy.sparse import vstack\n",
    "from nlp_utils import getTFV, getBOW\n",
    "from feat_utils import get_sample_indices_by_relevance, dump_feat_name\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "\n",
    "stats_feat_flag = True\n",
    "\n",
    "\n",
    "#####################\n",
    "## Helper function ##\n",
    "#####################\n",
    "## compute cosine similarity\n",
    "def cosine_sim(x, y):\n",
    "    try:\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print x\n",
    "        print y\n",
    "        d = 0.\n",
    "    return d\n",
    "\n",
    "## generate distance stats feat\n",
    "def generate_dist_stats_feat(metric, X_train, ids_train, X_test, ids_test, indices_dict, qids_test=None):\n",
    "    if metric == \"cosine\":\n",
    "        stats_feat = 0 * np.ones((len(ids_test), stats_feat_num*config.n_classes), dtype=float)\n",
    "        sim = 1. - pairwise_distances(X_test, X_train, metric=metric, n_jobs=1)\n",
    "    elif metric == \"euclidean\":\n",
    "        stats_feat = -1 * np.ones((len(ids_test), stats_feat_num*config.n_classes), dtype=float)\n",
    "        sim = pairwise_distances(X_test, X_train, metric=metric, n_jobs=1)\n",
    "\n",
    "    for i in range(len(ids_test)):\n",
    "        id = ids_test[i]\n",
    "        if qids_test is not None:\n",
    "            qid = qids_test[i]\n",
    "        for j in range(config.n_classes):\n",
    "            key = (qid, j+1) if qids_test is not None else j+1\n",
    "            if indices_dict.has_key(key):\n",
    "                inds = indices_dict[key]\n",
    "                # exclude this sample itself from the list of indices\n",
    "                inds = [ ind for ind in inds if id != ids_train[ind] ]\n",
    "                sim_tmp = sim[i][inds]\n",
    "                if len(sim_tmp) != 0:\n",
    "                    feat = [ func(sim_tmp) for func in stats_func ]\n",
    "                    ## quantile\n",
    "                    sim_tmp = pd.Series(sim_tmp)\n",
    "                    quantiles = sim_tmp.quantile(quantiles_range)\n",
    "                    feat = np.hstack((feat, quantiles))\n",
    "                    stats_feat[i,j*stats_feat_num:(j+1)*stats_feat_num] = feat\n",
    "    return stats_feat\n",
    "\n",
    "\n",
    "## extract all features\n",
    "def extract_feat(path, dfTrain, dfTest, mode, feat_names, column_names):\n",
    "\n",
    "    new_feat_names = copy(feat_names)\n",
    "    ## first fit a bow/tfidf on the all_text to get\n",
    "    ## the common vocabulary to ensure query/title/description\n",
    "    ## has the same length bow/tfidf for computing the similarity\n",
    "    if vocabulary_type == \"common\":\n",
    "        if vec_type == \"tfidf\":\n",
    "            vec = getTFV(ngram_range=ngram_range)\n",
    "        elif vec_type == \"bow\":\n",
    "            vec = getBOW(ngram_range=ngram_range)\n",
    "        vec.fit(dfTrain[\"all_text\"])\n",
    "        vocabulary = vec.vocabulary_\n",
    "    elif vocabulary_type == \"individual\":\n",
    "        vocabulary = None\n",
    "    for feat_name,column_name in zip(feat_names, column_names):\n",
    "\n",
    "        ##########################\n",
    "        ## basic bow/tfidf feat ##\n",
    "        ##########################\n",
    "        print \"generate %s feat for %s\" % (vec_type, column_name)\n",
    "        if vec_type == \"tfidf\":\n",
    "            vec = getTFV(ngram_range=ngram_range, vocabulary=vocabulary)\n",
    "        elif vec_type == \"bow\":\n",
    "            vec = getBOW(ngram_range=ngram_range, vocabulary=vocabulary)\n",
    "        X_train = vec.fit_transform(dfTrain[column_name])\n",
    "        X_test = vec.transform(dfTest[column_name])\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_train, f, -1)\n",
    "        with open(\"%s/%s.%s.feat.pkl\" % (path, mode, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_test, f, -1)\n",
    "        \n",
    "        if stats_feat_flag:\n",
    "            #####################################\n",
    "            ## bow/tfidf cosine sim stats feat ##\n",
    "            #####################################\n",
    "            ## get the indices of pooled samples\n",
    "            relevance_indices_dict = get_sample_indices_by_relevance(dfTrain)\n",
    "            query_relevance_indices_dict = get_sample_indices_by_relevance(dfTrain, \"qid\")\n",
    "            ## skip query part\n",
    "            if column_name in [\"product_title\", \"product_description\"]:\n",
    "                print \"generate %s stats feat for %s\" % (vec_type, column_name)\n",
    "                ## train\n",
    "                cosine_sim_stats_feat_by_relevance_train = generate_dist_stats_feat(\"cosine\", X_train, dfTrain[\"id\"].values,\n",
    "                                                                    X_train, dfTrain[\"id\"].values,\n",
    "                                                                    relevance_indices_dict)\n",
    "                cosine_sim_stats_feat_by_query_relevance_train = generate_dist_stats_feat(\"cosine\", X_train, dfTrain[\"id\"].values,\n",
    "                                                                            X_train, dfTrain[\"id\"].values,\n",
    "                                                                            query_relevance_indices_dict, dfTrain[\"qid\"].values)\n",
    "                with open(\"%s/train.%s_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(cosine_sim_stats_feat_by_relevance_train, f, -1)\n",
    "                with open(\"%s/train.%s_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(cosine_sim_stats_feat_by_query_relevance_train, f, -1)\n",
    "                ## test\n",
    "                cosine_sim_stats_feat_by_relevance_test = generate_dist_stats_feat(\"cosine\", X_train, dfTrain[\"id\"].values,\n",
    "                                                                    X_test, dfTest[\"id\"].values,\n",
    "                                                                    relevance_indices_dict)\n",
    "                cosine_sim_stats_feat_by_query_relevance_test = generate_dist_stats_feat(\"cosine\", X_train, dfTrain[\"id\"].values,\n",
    "                                                                            X_test, dfTest[\"id\"].values,\n",
    "                                                                            query_relevance_indices_dict, dfTest[\"qid\"].values)\n",
    "                with open(\"%s/%s.%s_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, mode, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(cosine_sim_stats_feat_by_relevance_test, f, -1)\n",
    "                with open(\"%s/%s.%s_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, mode, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(cosine_sim_stats_feat_by_query_relevance_test, f, -1)\n",
    "\n",
    "                ## update feat names\n",
    "                new_feat_names.append( \"%s_cosine_sim_stats_feat_by_relevance\" % feat_name )\n",
    "                new_feat_names.append( \"%s_cosine_sim_stats_feat_by_query_relevance\" % feat_name )\n",
    "\n",
    "\n",
    "    #####################\n",
    "    ## cosine sim feat ##\n",
    "    #####################\n",
    "    for i in range(len(feat_names)-1):\n",
    "        for j in range(i+1,len(feat_names)):\n",
    "            print \"generate common %s cosine sim feat for %s and %s\" % (vec_type, feat_names[i], feat_names[j])\n",
    "            for mod in [\"train\", mode]:\n",
    "                with open(\"%s/%s.%s.feat.pkl\" % (path, mod, feat_names[i]), \"rb\") as f:\n",
    "                    target_vec = cPickle.load(f)\n",
    "                with open(\"%s/%s.%s.feat.pkl\" % (path, mod, feat_names[j]), \"rb\") as f:\n",
    "                    obs_vec = cPickle.load(f)\n",
    "                sim = np.asarray(map(cosine_sim, target_vec, obs_vec))[:,np.newaxis]\n",
    "                ## dump feat\n",
    "                with open(\"%s/%s.%s_%s_%s_cosine_sim.feat.pkl\" % (path, mod, feat_names[i], feat_names[j], vec_type), \"wb\") as f:\n",
    "                    cPickle.dump(sim, f, -1)\n",
    "            ## update feat names\n",
    "            new_feat_names.append( \"%s_%s_%s_cosine_sim\" % (feat_names[i], feat_names[j], vec_type))\n",
    "\n",
    "\n",
    "    ##################\n",
    "    ## SVD features ##\n",
    "    ##################\n",
    "    ## we fit svd use stacked query/title/description bow/tfidf for further cosine simalirity computation\n",
    "    for i,feat_name in enumerate(feat_names):\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"rb\") as f:\n",
    "            X_vec_train = cPickle.load(f)\n",
    "        if i == 0:\n",
    "            X_vec_all_train = X_vec_train\n",
    "        else:\n",
    "            X_vec_all_train = vstack([X_vec_all_train, X_vec_train])\n",
    "\n",
    "    for n_components in svd_n_components:\n",
    "        svd = TruncatedSVD(n_components=n_components, n_iter=15)\n",
    "        svd.fit(X_vec_all_train)\n",
    "        ## load bow/tfidf (for less coding...)\n",
    "        for feat_name,column_name in zip(feat_names, column_names):\n",
    "            print \"generate common %s-svd%d feat for %s\" % (vec_type, n_components, column_name)\n",
    "            with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"rb\") as f:\n",
    "                X_vec_train = cPickle.load(f)\n",
    "            with open(\"%s/%s.%s.feat.pkl\" % (path, mode, feat_name), \"rb\") as f:\n",
    "                X_vec_test = cPickle.load(f)\n",
    "            X_svd_train = svd.transform(X_vec_train)\n",
    "            X_svd_test = svd.transform(X_vec_test)\n",
    "            with open(\"%s/train.%s_common_svd%d.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_svd_train, f, -1)\n",
    "            with open(\"%s/%s.%s_common_svd%d.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_svd_test, f, -1)\n",
    "\n",
    "            ## update feat names\n",
    "            new_feat_names.append( \"%s_common_svd%d\" % (feat_name, n_components) )\n",
    "            \n",
    "            if stats_feat_flag:\n",
    "                #####################################\n",
    "                ## bow/tfidf-svd cosine sim stats feat ##\n",
    "                #####################################\n",
    "                if column_name in [\"product_title\", \"product_description\"]:\n",
    "                    print \"generate common %s-svd%d stats feat for %s\" % (vec_type, n_components, column_name)\n",
    "                    ## train\n",
    "                    cosine_sim_stats_feat_by_relevance_train = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        relevance_indices_dict)\n",
    "                    cosine_sim_stats_feat_by_query_relevance_train = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                query_relevance_indices_dict, dfTrain[\"qid\"].values)\n",
    "                    with open(\"%s/train.%s_common_svd%d_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_relevance_train, f, -1)\n",
    "                    with open(\"%s/train.%s_common_svd%d_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_query_relevance_train, f, -1)\n",
    "                    ## test\n",
    "                    cosine_sim_stats_feat_by_relevance_test = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        X_svd_test, dfTest[\"id\"].values,\n",
    "                                                                        relevance_indices_dict)\n",
    "                    cosine_sim_stats_feat_by_query_relevance_test = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                X_svd_test, dfTest[\"id\"].values,\n",
    "                                                                                query_relevance_indices_dict, dfTest[\"qid\"].values)\n",
    "                    with open(\"%s/%s.%s_common_svd%d_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_relevance_test, f, -1)\n",
    "                    with open(\"%s/%s.%s_common_svd%d_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_query_relevance_test, f, -1)\n",
    "\n",
    "                    ## update feat names\n",
    "                    new_feat_names.append( \"%s_common_svd%d_cosine_sim_stats_feat_by_relevance\" % (feat_name, n_components) )\n",
    "                    new_feat_names.append( \"%s_common_svd%d_cosine_sim_stats_feat_by_query_relevance\" % (feat_name, n_components) )\n",
    "\n",
    "\n",
    "        #####################\n",
    "        ## cosine sim feat ##\n",
    "        #####################\n",
    "        for i in range(len(feat_names)-1):\n",
    "            for j in range(i+1,len(feat_names)):\n",
    "                print \"generate common %s-svd%d cosine sim feat for %s and %s\" % (vec_type, n_components, feat_names[i], feat_names[j])\n",
    "                for mod in [\"train\", mode]:\n",
    "                    with open(\"%s/%s.%s_common_svd%d.feat.pkl\" % (path, mod, feat_names[i], n_components), \"rb\") as f:\n",
    "                        target_vec = cPickle.load(f)\n",
    "                    with open(\"%s/%s.%s_common_svd%d.feat.pkl\" % (path, mod, feat_names[j], n_components), \"rb\") as f:\n",
    "                        obs_vec = cPickle.load(f)\n",
    "                    sim = np.asarray(map(cosine_sim, target_vec, obs_vec))[:,np.newaxis]\n",
    "                    ## dump feat\n",
    "                    with open(\"%s/%s.%s_%s_%s_common_svd%d_cosine_sim.feat.pkl\" % (path, mod, feat_names[i], feat_names[j], vec_type, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(sim, f, -1)\n",
    "                ## update feat names\n",
    "                new_feat_names.append( \"%s_%s_%s_common_svd%d_cosine_sim\" % (feat_names[i], feat_names[j], vec_type, n_components))\n",
    "\n",
    "        #########################\n",
    "        ## Individual SVD feat ##\n",
    "        #########################\n",
    "        ## generate individual svd feat\n",
    "        for feat_name,column_name in zip(feat_names, column_names):\n",
    "            print \"generate individual %s-svd%d feat for %s\" % (vec_type, n_components, column_name)\n",
    "            with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"rb\") as f:\n",
    "                X_vec_train = cPickle.load(f)\n",
    "            with open(\"%s/%s.%s.feat.pkl\" % (path, mode, feat_name), \"rb\") as f:\n",
    "                X_vec_test = cPickle.load(f)\n",
    "            svd = TruncatedSVD(n_components=n_components, n_iter=15)\n",
    "            X_svd_train = svd.fit_transform(X_vec_train)\n",
    "            X_svd_test = svd.transform(X_vec_test)\n",
    "            with open(\"%s/train.%s_individual_svd%d.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_svd_train, f, -1)\n",
    "            with open(\"%s/%s.%s_individual_svd%d.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_svd_test, f, -1)\n",
    "            ## update feat names\n",
    "            new_feat_names.append( \"%s_individual_svd%d\" % (feat_name, n_components) )\n",
    "\n",
    "            if stats_feat_flag:\n",
    "                #########################################\n",
    "                ## bow/tfidf-svd cosine sim stats feat ##\n",
    "                #########################################\n",
    "                if column_name in [\"product_title\", \"product_description\"]:\n",
    "                    print \"generate individual %s-svd%d stats feat for %s\" % (vec_type, n_components, column_name)\n",
    "                    ## train\n",
    "                    cosine_sim_stats_feat_by_relevance_train = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        relevance_indices_dict)\n",
    "                    cosine_sim_stats_feat_by_query_relevance_train = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                query_relevance_indices_dict, dfTrain[\"qid\"].values)\n",
    "                    with open(\"%s/train.%s_individual_svd%d_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_relevance_train, f, -1)\n",
    "                    with open(\"%s/train.%s_individual_svd%d_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_query_relevance_train, f, -1)\n",
    "                    ## test\n",
    "                    cosine_sim_stats_feat_by_relevance_test = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                        X_svd_test, dfTest[\"id\"].values,\n",
    "                                                                        relevance_indices_dict)\n",
    "                    cosine_sim_stats_feat_by_query_relevance_test = generate_dist_stats_feat(\"cosine\", X_svd_train, dfTrain[\"id\"].values,\n",
    "                                                                                X_svd_test, dfTest[\"id\"].values,\n",
    "                                                                                query_relevance_indices_dict, dfTest[\"qid\"].values)\n",
    "                    with open(\"%s/%s.%s_individual_svd%d_cosine_sim_stats_feat_by_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_relevance_test, f, -1)\n",
    "                    with open(\"%s/%s.%s_individual_svd%d_cosine_sim_stats_feat_by_query_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                        cPickle.dump(cosine_sim_stats_feat_by_query_relevance_test, f, -1)\n",
    "\n",
    "                    ## update feat names\n",
    "                    new_feat_names.append( \"%s_individual_svd%d_cosine_sim_stats_feat_by_relevance\" % (feat_name, n_components) )\n",
    "                    new_feat_names.append( \"%s_individual_svd%d_cosine_sim_stats_feat_by_query_relevance\" % (feat_name, n_components) )\n",
    "\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    ## bow/tfidf-tsne feat ##\n",
    "    #########################\n",
    "    ## generate t-sne feat\n",
    "    for n_components in tsne_n_components:\n",
    "        for feat_name,column_name in zip(feat_names, column_names):\n",
    "            print \"generate individual %s-tsne%d feat for %s\" % (vec_type, n_components, column_name)\n",
    "            with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"rb\") as f:\n",
    "                X_vec_train = cPickle.load(f)\n",
    "            with open(\"%s/%s.%s.feat.pkl\" % (path, mode, feat_name), \"rb\") as f:\n",
    "                X_vec_test = cPickle.load(f)\n",
    "            tsne = TSNE(n_components=n_components, init='pca', random_state=2015, metric=\"cosine\")\n",
    "            X = vstack([X_vec_train, X_vec_test])\n",
    "            Y = tsne.fit_transform(X)\n",
    "            num_train = X_vec_train.shape[0]\n",
    "            X_tsne_train = Y[:num_train]\n",
    "            X_tsne_test = Y[num_train:]\n",
    "            with open(\"%s/train.%s_individual_tsne%d.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_tsne_train, f, -1)\n",
    "            with open(\"%s/%s.%s_individual_tsne%d.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                cPickle.dump(X_tsne_test, f, -1)\n",
    "\n",
    "            ##################################################\n",
    "            ## bow/tfidf-tsne euclidean distance stats feat ##\n",
    "            ##################################################\n",
    "            if column_name in [\"product_title\", \"product_description\"]:\n",
    "                print \"generate individual %s-tsne%d stats feat for %s\" % (vec_type, n_components, column_name)\n",
    "                ## train\n",
    "                euclidean_dist_stats_feat_by_relevance_train = generate_dist_stats_feat(\"euclidean\", X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                    X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                    relevance_indices_dict)\n",
    "                euclidean_dist_stats_feat_by_query_relevance_train = generate_dist_stats_feat(\"euclidean\", X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                            X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                            query_relevance_indices_dict, dfTrain[\"qid\"].values)\n",
    "                with open(\"%s/train.%s_individual_tsne%d_euclidean_dist_stats_feat_by_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                    cPickle.dump(euclidean_dist_stats_feat_by_relevance_train, f, -1)\n",
    "                with open(\"%s/train.%s_individual_tsne%d_euclidean_dist_stats_feat_by_query_relevance.feat.pkl\" % (path, feat_name, n_components), \"wb\") as f:\n",
    "                    cPickle.dump(euclidean_dist_stats_feat_by_query_relevance_train, f, -1)\n",
    "                ## test\n",
    "                euclidean_dist_stats_feat_by_relevance_test = generate_dist_stats_feat(\"euclidean\", X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                    X_tsne_test, dfTest[\"id\"].values,\n",
    "                                                                    relevance_indices_dict)\n",
    "                euclidean_dist_stats_feat_by_query_relevance_test = generate_dist_stats_feat(\"euclidean\", X_tsne_train, dfTrain[\"id\"].values,\n",
    "                                                                            X_tsne_test, dfTest[\"id\"].values,\n",
    "                                                                            query_relevance_indices_dict, dfTest[\"qid\"].values)\n",
    "                with open(\"%s/%s.%s_individual_tsne%d_euclidean_dist_stats_feat_by_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                    cPickle.dump(euclidean_dist_stats_feat_by_relevance_test, f, -1)\n",
    "                with open(\"%s/%s.%s_individual_tsne%d_euclidean_dist_stats_feat_by_query_relevance.feat.pkl\" % (path, mode, feat_name, n_components), \"wb\") as f:\n",
    "                    cPickle.dump(euclidean_dist_stats_feat_by_query_relevance_test, f, -1)\n",
    "\n",
    "                ## update feat names\n",
    "                new_feat_names.append( \"%s_individual_tsne%d_euclidean_dist_stats_feat_by_relevance\" % (feat_name, n_components) )\n",
    "                new_feat_names.append( \"%s_individual_tsne%d_euclidean_dist_stats_feat_by_query_relevance\" % (feat_name, n_components) )\n",
    "    \"\"\"\n",
    "\n",
    "    return new_feat_names\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ############\n",
    "    ## Config ##\n",
    "    ############\n",
    "    ## stats to extract\n",
    "    quantiles_range = np.arange(0, 1.5, 0.5)\n",
    "    stats_func = [ np.mean, np.std ]\n",
    "    stats_feat_num = len(quantiles_range) + len(stats_func)\n",
    "\n",
    "    ## tfidf config\n",
    "    vec_types = [ \"tfidf\", \"bow\" ]\n",
    "    ngram_range = config.basic_tfidf_ngram_range\n",
    "    vocabulary_type = config.basic_tfidf_vocabulary_type\n",
    "    svd_n_components = [100, 150]\n",
    "    tsne_n_components = [2]\n",
    "\n",
    "    ## feat name config\n",
    "    column_names = [ \"query\", \"product_title\", \"product_description\" ]\n",
    "\n",
    "    ###############\n",
    "    ## Load Data ##\n",
    "    ###############\n",
    "    ## load data\n",
    "    with open(config.processed_train_data_path, \"rb\") as f:\n",
    "        dfTrain = cPickle.load(f)\n",
    "    with open(config.processed_test_data_path, \"rb\") as f:\n",
    "        dfTest = cPickle.load(f)\n",
    "    ## load pre-defined stratified k-fold index\n",
    "    with open(\"%s/stratifiedKFold.%s.pkl\" % (config.data_folder, config.stratified_label), \"rb\") as f:\n",
    "            skf = cPickle.load(f)\n",
    "\n",
    "    ## for fitting common vocabulary\n",
    "    def cat_text(x):\n",
    "        res = '%s %s %s' % (x['query'], x['product_title'], x['product_description'])\n",
    "        return res\n",
    "    dfTrain[\"all_text\"] = list(dfTrain.apply(cat_text, axis=1))\n",
    "    dfTest[\"all_text\"] = list(dfTest.apply(cat_text, axis=1))\n",
    "\n",
    "    for vec_type in vec_types:\n",
    "        ## save feat names\n",
    "        feat_names = [ \"query\", \"title\", \"description\" ]\n",
    "        feat_names = [ name+\"_%s_%s_vocabulary\" % (vec_type, vocabulary_type) for name in feat_names ]\n",
    "        ## file to save feat names\n",
    "        feat_name_file = \"%s/basic_%s_and_cosine_sim.feat_name\" % (config.feat_folder, vec_type)\n",
    "\n",
    "        #######################\n",
    "        ## Generate Features ##\n",
    "        #######################\n",
    "        print(\"==================================================\")\n",
    "        print(\"Generate basic %s features...\" % vec_type)\n",
    "\n",
    "        print(\"For cross-validation...\")\n",
    "        for run in range(config.n_runs):\n",
    "            ## use 33% for training and 67 % for validation\n",
    "            ## so we switch trainInd and validInd\n",
    "            for fold, (validInd, trainInd) in enumerate(skf[run]):\n",
    "                print(\"Run: %d, Fold: %d\" % (run+1, fold+1))\n",
    "                path = \"%s/Run%d/Fold%d\" % (config.feat_folder, run+1, fold+1)\n",
    "                \n",
    "                dfTrain2 = dfTrain.iloc[trainInd].copy()\n",
    "                dfValid = dfTrain.iloc[validInd].copy()\n",
    "                ## extract feat\n",
    "                extract_feat(path, dfTrain2, dfValid, \"valid\", feat_names, column_names)\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"For training and testing...\")\n",
    "        path = \"%s/All\" % config.feat_folder\n",
    "        ## extract feat\n",
    "        feat_names = extract_feat(path, dfTrain, dfTest, \"test\", feat_names, column_names)\n",
    "        ## dump feat name\n",
    "        dump_feat_name(feat_names, feat_name_file)\n",
    "\n",
    "        print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load genFeat_cooccurrence_tfidf_feat.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    genFeat_cooccurrence_tfidf.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file generates the following features for each run and fold, and for the entire training and testing set.\n",
    "\n",
    "        1. tfidf for the following cooccurrence terms\n",
    "            - query unigram/bigram & title unigram/bigram\n",
    "            - query unigram/bigram & description unigram/bigram\n",
    "            - query id & title unigram/bigram\n",
    "            - query id & description unigram/bigram\n",
    "\n",
    "        2. corresponding lsa (svd) version features\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import cPickle\n",
    "import ngram\n",
    "from feat_utils import dump_feat_name\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nlp_utils import stopwords, english_stemmer, stem_tokens, getTFV\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "######################\n",
    "## Pre-process data ##\n",
    "######################\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "#token_pattern = r'\\w{1,}'\n",
    "#token_pattern = r\"\\w+\"\n",
    "#token_pattern = r\"[\\w']+\"\n",
    "def preprocess_data(line,\n",
    "                    token_pattern=token_pattern,\n",
    "                    exclude_stopword=config.cooccurrence_word_exclude_stopword,\n",
    "                    encode_digit=False):\n",
    "    token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)\n",
    "    ## tokenize\n",
    "    tokens = [x.lower() for x in token_pattern.findall(line)]\n",
    "    ## stem\n",
    "    tokens_stemmed = stem_tokens(tokens, english_stemmer)\n",
    "    if exclude_stopword:\n",
    "        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "########################\n",
    "## Cooccurrence terms ##\n",
    "########################\n",
    "def cooccurrence_terms(lst1, lst2, join_str):\n",
    "    terms = [\"\"] * len(lst1) * len(lst2)\n",
    "    cnt =  0\n",
    "    for item1 in lst1:\n",
    "        for item2 in lst2:\n",
    "            terms[cnt] = item1 + join_str + item2\n",
    "            cnt += 1\n",
    "    res = \" \".join(terms)\n",
    "    return res\n",
    "\n",
    "\n",
    "##################\n",
    "## Extract feat ##\n",
    "##################\n",
    "def extract_feat(df):\n",
    "    ## unigram\n",
    "    print \"generate unigram\"\n",
    "    df[\"query_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"query\"]), axis=1))\n",
    "    df[\"title_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_title\"]), axis=1))\n",
    "    df[\"description_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_description\"]), axis=1))\n",
    "    ## bigram\n",
    "    print \"generate bigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    # ## trigram\n",
    "    # join_str = \"_\"\n",
    "    # df[\"query_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    # df[\"title_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    # df[\"description_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "\n",
    "    ## cooccurrence terms\n",
    "    join_str = \"X\"\n",
    "    # query unigram\n",
    "    df[\"query_unigram_title_unigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_unigram\"], x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"query_unigram_title_bigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_unigram\"], x[\"title_bigram\"], join_str), axis=1))\n",
    "    df[\"query_unigram_description_unigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_unigram\"], x[\"description_unigram\"], join_str), axis=1))\n",
    "    df[\"query_unigram_description_bigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_unigram\"], x[\"description_bigram\"], join_str), axis=1))\n",
    "    # query bigram\n",
    "    df[\"query_bigram_title_unigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_bigram\"], x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"query_bigram_title_bigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_bigram\"], x[\"title_bigram\"], join_str), axis=1))\n",
    "    df[\"query_bigram_description_unigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_bigram\"], x[\"description_unigram\"], join_str), axis=1))\n",
    "    df[\"query_bigram_description_bigram\"] = list(df.apply(lambda x: cooccurrence_terms(x[\"query_bigram\"], x[\"description_bigram\"], join_str), axis=1))\n",
    "    # query id\n",
    "    df[\"query_id_title_unigram\"] = list(df.apply(lambda x: cooccurrence_terms([\"qid\"+str(x[\"qid\"])], x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"query_id_title_bigram\"] = list(df.apply(lambda x: cooccurrence_terms([\"qid\"+str(x[\"qid\"])], x[\"title_bigram\"], join_str), axis=1))\n",
    "    df[\"query_id_description_unigram\"] = list(df.apply(lambda x: cooccurrence_terms([\"qid\"+str(x[\"qid\"])], x[\"description_unigram\"], join_str), axis=1))\n",
    "    df[\"query_id_description_bigram\"] = list(df.apply(lambda x: cooccurrence_terms([\"qid\"+str(x[\"qid\"])], x[\"description_bigram\"], join_str), axis=1))\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ############\n",
    "    ## Config ##\n",
    "    ############\n",
    "    ## cooccurrence terms column names\n",
    "    column_names = [\n",
    "        \"query_unigram_title_unigram\",\n",
    "        \"query_unigram_title_bigram\",\n",
    "        \"query_unigram_description_unigram\",\n",
    "        \"query_unigram_description_bigram\",\n",
    "        \"query_bigram_title_unigram\",\n",
    "        \"query_bigram_title_bigram\",\n",
    "        \"query_bigram_description_unigram\",\n",
    "        \"query_bigram_description_bigram\",\n",
    "        \"query_id_title_unigram\",\n",
    "        \"query_id_title_bigram\",\n",
    "        \"query_id_description_unigram\",\n",
    "        \"query_id_description_bigram\",\n",
    "    ]\n",
    "    ## feature names\n",
    "    feat_names = [ name+\"_tfidf\" for name in column_names ]\n",
    "    ## file to save feat names\n",
    "    feat_name_file = \"%s/intersect_tfidf.feat_name\" % config.feat_folder\n",
    "\n",
    "    ngram_range = config.cooccurrence_tfidf_ngram_range\n",
    "\n",
    "    svd_n_components = 100\n",
    "\n",
    "    ###############\n",
    "    ## Load Data ##\n",
    "    ###############\n",
    "    ## load data\n",
    "    with open(config.processed_train_data_path, \"rb\") as f:\n",
    "        dfTrain = cPickle.load(f)\n",
    "    with open(config.processed_test_data_path, \"rb\") as f:\n",
    "        dfTest = cPickle.load(f)\n",
    "    ## load pre-defined stratified k-fold index\n",
    "    with open(\"%s/stratifiedKFold.%s.pkl\" % (config.data_folder, config.stratified_label), \"rb\") as f:\n",
    "            skf = cPickle.load(f)\n",
    "\n",
    "    #######################\n",
    "    ## Generate Features ##\n",
    "    #######################\n",
    "    print(\"==================================================\")\n",
    "    print(\"Generate co-occurrence tfidf features...\")\n",
    "\n",
    "    ## get cooccurrence terms\n",
    "    extract_feat(dfTrain)\n",
    "    extract_feat(dfTest)\n",
    "\n",
    "    ######################\n",
    "    ## Cross validation ##\n",
    "    ######################\n",
    "    print(\"For cross-validation...\")\n",
    "    for run in range(config.n_runs):\n",
    "        ## use 33% for training and 67 % for validation\n",
    "        ## so we switch trainInd and validInd\n",
    "        for fold, (validInd, trainInd) in enumerate(skf[run]):\n",
    "            print(\"Run: %d, Fold: %d\" % (run+1, fold+1))\n",
    "            path = \"%s/Run%d/Fold%d\" % (config.feat_folder, run+1, fold+1)\n",
    "                \n",
    "            for feat_name,column_name in zip(feat_names, column_names):\n",
    "                print \"generate %s feat\" % feat_name\n",
    "                ## tfidf\n",
    "                tfv = getTFV(ngram_range=ngram_range)\n",
    "                X_tfidf_train = tfv.fit_transform(dfTrain.iloc[trainInd][column_name])\n",
    "                X_tfidf_valid = tfv.transform(dfTrain.iloc[validInd][column_name])\n",
    "                with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_tfidf_train, f, -1)\n",
    "                with open(\"%s/valid.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_tfidf_valid, f, -1)\n",
    "\n",
    "                ## svd\n",
    "                svd = TruncatedSVD(n_components=svd_n_components, n_iter=15)\n",
    "                X_svd_train = svd.fit_transform(X_tfidf_train)\n",
    "                X_svd_test = svd.transform(X_tfidf_valid)\n",
    "                with open(\"%s/train.%s_individual_svd%d.feat.pkl\" % (path, feat_name, svd_n_components), \"wb\") as f:\n",
    "                    cPickle.dump(X_svd_train, f, -1)\n",
    "                with open(\"%s/valid.%s_individual_svd%d.feat.pkl\" % (path, feat_name, svd_n_components), \"wb\") as f:\n",
    "                    cPickle.dump(X_svd_test, f, -1)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "    #################\n",
    "    ## Re-training ##\n",
    "    #################\n",
    "    print(\"For training and testing...\")\n",
    "    path = \"%s/All\" % config.feat_folder\n",
    "    for feat_name,column_name in zip(feat_names, column_names):\n",
    "        print \"generate %s feat\" % feat_name\n",
    "        tfv = getTFV(ngram_range=ngram_range)\n",
    "        X_tfidf_train = tfv.fit_transform(dfTrain[column_name])\n",
    "        X_tfidf_test = tfv.transform(dfTest[column_name])\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_tfidf_train, f, -1)\n",
    "        with open(\"%s/test.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_tfidf_test, f, -1)\n",
    "\n",
    "        ## svd\n",
    "        svd = TruncatedSVD(n_components=svd_n_components, n_iter=15)\n",
    "        X_svd_train = svd.fit_transform(X_tfidf_train)\n",
    "        X_svd_test = svd.transform(X_tfidf_test)\n",
    "        with open(\"%s/train.%s_individual_svd%d.feat.pkl\" % (path, feat_name, svd_n_components), \"wb\") as f:\n",
    "            cPickle.dump(X_svd_train, f, -1)\n",
    "        with open(\"%s/test.%s_individual_svd%d.feat.pkl\" % (path, feat_name, svd_n_components), \"wb\") as f:\n",
    "            cPickle.dump(X_svd_test, f, -1)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "    ## save feat names\n",
    "    print(\"Feature names are stored in %s\" % feat_name_file)\n",
    "    feat_names += [ \"%s_individual_svd%d\"%(f, svd_n_components) for f in feat_names ]\n",
    "    dump_feat_name(feat_names, feat_name_file)\n",
    "\n",
    "    print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = '''git add -A\n",
    "git commit -m\"add data\"\n",
    "git push'''\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
